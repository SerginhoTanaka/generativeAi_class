{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import faiss\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from uuid import uuid4\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrega todos os arquivos PDF do diretório \"data\" e os divide em documentos individuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "data_dir = \"data\"\n",
    "loader = PyPDFDirectoryLoader(data_dir)\n",
    "documents = loader.load_and_split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciando a LLM e a classe de Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(openai_api_key=api_key,model=\"gpt-3.5-turbo\")\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **index = faiss.IndexFlatL2(len(embeddings.embed_query(\"example query\")))**\n",
    "- `faiss.IndexFlatL2()`: Essa é uma função da biblioteca FAISS (Facebook AI Similarity Search) que cria um índice de vetor plano usando a distância L2 (Euclidiana) como métrica de similaridade.\n",
    "\n",
    "- `len(embeddings.embed_query(\"example query\"))`: Essa parte do código chama o método `embed_query()` da classe de embeddings (por exemplo, `OpenAIEmbeddings` ou `HuggingFaceEmbeddings`) e retorna o comprimento do vetor de embedding resultante. Isso é usado para definir o número de dimensões do índice FAISS.\n",
    "\n",
    "Então, essa linha de código está criando um índice FAISS que usa a distância L2 como métrica de similaridade e tem um número de dimensões igual ao comprimento do vetor de embedding retornado pela chamada `embeddings.embed_query(\"example query\")`.\n",
    "\n",
    "Esse índice FAISS pode ser usado posteriormente para realizar buscas de similaridade em um conjunto de documentos vetorizados. O índice FAISS é otimizado para realizar essas buscas de maneira eficiente, mesmo em conjuntos de dados muito grandes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"example query\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração e Mapeamento de Documentos para Armazenamento e Busca de Similaridade com FAISS\n",
    "- `index_to_docstore_id = {i: str(uuid4()) for i in range(len(documents))}`:\n",
    "  - Essa linha cria um dicionário que mapeia o índice de cada documento em `documents` para um UUID (Universally Unique Identifier) único.\n",
    "  - O `uuid4()` gera um UUID aleatório e único para cada documento.\n",
    "  - Esse mapeamento será usado posteriormente para conectar os documentos aos seus respectivos vetores de embedding no armazenamento de vetores.\n",
    "\n",
    "- `docstore = InMemoryDocstore({index_to_docstore_id[i]: doc for i, doc in enumerate(documents)})`:\n",
    "  - Essa linha cria uma instância da classe `InMemoryDocstore`, que é um armazenamento de documentos em memória.\n",
    "  - O dicionário criado anteriormente (`index_to_docstore_id`) é usado para mapear cada documento em `documents` para seu respectivo UUID no armazenamento de documentos.\n",
    "\n",
    "- `vector_store = FAISS(embedding_function=embeddings, index=index, docstore=docstore, index_to_docstore_id=index_to_docstore_id)`:\n",
    "  - Essa linha cria uma instância da classe `FAISS`, que é um armazenamento de vetores baseado na biblioteca FAISS.\n",
    "  - `embedding_function=embeddings`: Define a função de embedding a ser usada para gerar os vetores de embedding dos documentos.\n",
    "  - `index=index`: Define o índice FAISS criado anteriormente.\n",
    "  - `docstore=docstore`: Define o armazenamento de documentos a ser usado.\n",
    "  - `index_to_docstore_id=index_to_docstore_id`: Define o mapeamento entre os índices dos documentos e seus respectivos UUIDs no armazenamento de documentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_docstore_id = {i: str(uuid4()) for i in range(len(documents))}\n",
    "docstore = InMemoryDocstore({index_to_docstore_id[i]: doc for i, doc in enumerate(documents)})\n",
    "vector_store = FAISS(embedding_function=embeddings, index=index, docstore=docstore, index_to_docstore_id=index_to_docstore_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adiciona os documentos no VDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", len(vector_store.docstore._dict), \"docs in the collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escervendo os *PROMPTS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_question_system_template = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *HistoryAwareRetriever* é um tipo especial de recuperador (retriever) que leva em conta o histórico da conversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_question_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", condense_question_system_template),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, vector_store.as_retriever(), condense_question_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **StuffDocumentsChain**:\n",
    "  - É um tipo de cadeia (chain) que passa uma lista de documentos para um modelo de linguagem (LLM) usando um prompt específico, como `qa_prompt`.\n",
    "  - Essa cadeia é projetada para \"encher\" o prompt com os documentos fornecidos, permitindo que o modelo de linguagem processe e gere respostas com base nas informações desses documentos.\n",
    "\n",
    "- **RetrievalChain**:\n",
    "  - É um tipo de cadeia que combina um recuperador (retriever) e uma cadeia de perguntas e respostas (Q&A).\n",
    "  - O recuperador é responsável por buscar documentos relevantes de uma base de dados ou de um conjunto de documentos.\n",
    "  - A cadeia de perguntas e respostas, então, utiliza esses documentos recuperados para gerar respostas precisas para as perguntas feitas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "convo_qa_chain = create_retrieval_chain(history_aware_retriever, qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = convo_qa_chain.invoke({\"input\": \"Me de um resumo do artigo\", \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O artigo discute a aplicação da Lei com foco na justiça, reconhecendo que a perfeição da Lei é inatingível, mas defendendo que sua aplicação deve ser justa. Mostra visualizações de atenção em camadas específicas de um modelo, destacando como certas cabeças de atenção lidam com resolução de anáfora e estrutura frasal. Além disso, cita referências de trabalhos anteriores sobre processamento de linguagem natural e aprendizado de máquina.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
